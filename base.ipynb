{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1,2,3,4]).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4], dtype=torch.int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(np.array([1,2,3,4])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(np.array([1,2,3,4])).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor([1.0, 2.0, 3.0, 4.0]).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(np.array([1.0, 2.0, 3.0, 4.0])).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor([[1,2], [3,4,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor([[1,2,3], [4,5,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randn(3,3).to(torch.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randint(0, 5, (3,3)).to(torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.rand(3,3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randn(2,3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.zeros(2,2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.ones(1,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.eye(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randint(0, 10, (3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.randn(3,3)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.zeros_like(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.ones_like(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.rand_like(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randn_like(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.new_tensor([1,2,3]).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.new_zeros(3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.new_ones(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randn(3, 3, device=\"cpu\") # 获取存储在CPU上的一个张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randn(3, 3, device=\"cuda:0\") # 获取存储在0号GPU上的一个张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randn(3, 3, device=\"cuda:1\") # 获取存储在1号GPU上的一个张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randn(3, 3, device=\"cuda:0\").device # 获取当前张量的设备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randn(3, 3, device=\"cuda:0\").cpu().device # 张量从1号GPU转移到CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randn(3, 3, device=\"cuda:0\").cuda(0).device # 张量从1号GPU转移到0号GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randn(3, 3, device=\"cuda:1\").to(\"cuda:0\").device # 张量从1号GPU转移到0号GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.randn(3,4,5) # 产生一个3×4×5的张量\n",
    "t.ndimension() # 获取维度的数目\n",
    "t.nelement() # 获取该张量的总元素数目\n",
    "t.size() # 获取该张量每个维度的大小，调用方法\n",
    "t.shape # 获取该张量每个维度的大小，访问属性\n",
    "t.size(0) # 获取该张量维度0的大小，调用方法\n",
    "t = torch.randn(12) # 产生大小为12的向量\n",
    "t.view(3, 4) # 向量改变形状为3×4的矩阵\n",
    "t.view(4, 3) # 向量改变形状为4×3的矩阵\n",
    "t.view(-1, 4) # 第一个维度为-1，PyTorch会自动计算该维度的具体值\n",
    "t # view方法不改变底层数据，改变view后张量会改变原来的张量\n",
    "t.view(4, 3)[0, 0] = 1.0\n",
    "t.data_ptr() # 获取张量的数据指针\n",
    "t.view(3,4).data_ptr() # 数据指针不改变\n",
    "t.view(4,3).data_ptr() # 同上，不改变\n",
    "t.view(3,4).contiguous().data_ptr() # 同上，不改变\n",
    "t.view(4,3).contiguous().data_ptr() # 同上，不改变\n",
    "t.view(3,4).transpose(0,1).data_ptr() # transpose方法交换两个维度的步长\n",
    "t.view(3,4).transpose(0,1).contiguous().data_ptr() # 步长和维度不兼容，重新生成张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.randn(2,3,4) # 构造2×3×4的张量\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[1,2,3] # 取张量在0维1号、1维2号、2维3号的元素（编号从0开始）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[:,1:-1,1:3] # 仅仅一个冒号表示取所有的，-1表示最后一个元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "t[1,2,3] = -10\n",
    "t # 直接更改索引和切片会更改原始张量的值\n",
    "t > 0 # 张量大于零部分的掩码\n",
    "t[t>0] # 根据掩码选择张量的元素，注意最后选出来的是一个向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.rand(3, 4) # 产生一个3×4的张量\n",
    "t1 # 打印张量的值\n",
    "t1.sqrt() # 张量的平方根，张量内部方法\n",
    "torch.sqrt(t1) # 张量的平方根，函数形式\n",
    "t1 # 前两个操作不改变张量的值\n",
    "t1.sqrt_() # 平方根原地操作\n",
    "t1 # 原地操作，改变张量的值\n",
    "torch.sum(t1) # 默认对所有的元素求和\n",
    "torch.sum(t1, 0) # 对第0维的元素求和\n",
    "torch.sum(t1, [0,1]) # 对第0、1维的元素求和\n",
    "t1.mean() # 对所有元素求平均，也可以用torch.mean函数\n",
    "t1.mean(0) # 对第0维的元素求平均\n",
    "torch.mean(t1, [0,1]) # 对第0、1维元素求平均"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.rand(2, 3)\n",
    "t2 = torch.rand(2, 3)\n",
    "t1.add(t2) # 四则运算，不改变参与运算的张量的值\n",
    "t1+t2\n",
    "t1.sub(t2)\n",
    "t1-t2\n",
    "t1.mul(t2)\n",
    "t1*t2\n",
    "t1.div(t2)\n",
    "t1/t2\n",
    "t1\n",
    "t1.add_(t2) # 四则运算，改变参与运算张量的值\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.randn(3,4) # 建立一个3×4的张量\n",
    "t\n",
    "torch.argmax(t, 0) # 函数调用，返回的是沿着第0个维度，极大值所在位置\n",
    "t.argmin(1) # 内置方法调用，返回的是沿着第1个维度，极小值所在的位置\n",
    "torch.max(t, -1) # 函数调用，返回的是沿着最后一个维度，包含极大值和极大值所在位置的元组\n",
    "t.min(0) # 内置方法调用，返回的是沿着第0个维度，包含极小值和极小值所在位置的元组\n",
    "t.sort(-1) # 沿着最后一个维度排序，返回排序后的张量和张量元素在该维度的原始位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(3,4) # 建立一个3×4的张量\n",
    "b = torch.randn(4,3) # 建立一个4×3的张量\n",
    "torch.mm(a,b) # 矩阵乘法，调用函数，返回3×3的矩阵乘积\n",
    "a.mm(b) # 矩阵乘法，内置方法\n",
    "a@b # 矩阵乘法，@运算符号\n",
    "a = torch.randn(2,3,4) # 建立一个大小为2×3×4的张量\n",
    "b = torch.randn(2,4,3) # 建立一个张量，大小为2×4×3\n",
    "torch.bmm(a,b) # （迷你）批次矩阵乘法，返回结果为2×3×3，函数形式\n",
    "a.bmm(b) # 同上乘法，内置方法形式\n",
    "a@b # 运算符号形式，根据输入张量的形状决定调用批次矩阵乘法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(2,3,4) # 随机产生张量\n",
    "b = torch.randn(2,4,3)\n",
    "a.bmm(b) # 批次矩阵乘法的结果\n",
    "torch.einsum(\"bnk,bkl->bnl\", a, b) # einsum函数的结果，和前面的结果一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.randn(3,4) # 随机产生四个张量\n",
    "t2 = torch.randn(3,4)\n",
    "t3 = torch.randn(3,4)\n",
    "t4 = torch.randn(3,2) # 沿着最后一个维度做堆叠，返回大小为3×4×3的张量\n",
    "torch.stack([t1,t2,t3], -1).shape\n",
    "torch.cat([t1,t2,t3,t4], -1).shape # 沿着最后一个维度做拼接，返回大小为3×14的张量\n",
    "t = torch.randn(3, 6) # 随机产生一个3×6的张量\n",
    "t.split([1,2,3], -1) # 把张量沿着最后一个维度分割为三个张量\n",
    "t.split(3, -1) # 把张量沿着最后一个维度分割，分割大小为3，输出的张量大小均为3×3\n",
    "t.chunk(3, -1) # 把张量沿着最后一个维度分割为三个张量，大小均为3×2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.rand(3, 4) # 随机生成一个张量\n",
    "t.shape\n",
    "t.unsqueeze(-1).shape # 扩增最后一个维度\n",
    "t.unsqueeze(-1).unsqueeze(-1).shape # 继续扩增最后一个维度\n",
    "t = torch.rand(1,3,4,1) # 随机生成一个张量，有两个维度大小为1\n",
    "t.shape\n",
    "t.squeeze().shape # 压缩所有大小为1的维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.randn(3,4,5) # 定义3×4×5的张量1\n",
    "t2 = torch.randn(3,5) # 定义 3×5的张量2\n",
    "t1\n",
    "t2\n",
    "t2 = t2.unsqueeze(1) # 张量2的形状变为3×1×5\n",
    "t1.shape\n",
    "t2.shape\n",
    "t3 = t1 + t2 # 广播求和，最后结果为3×4×5的张量\n",
    "t3\n",
    "t3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-1f99770d1500>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-9-1f99770d1500>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    def __init__(self, ...): # 定义类的初始化函数，...是用户的传入参数\u001b[0m\n\u001b[1;37m                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, ...): # 定义类的初始化函数，...是用户的传入参数\n",
    "        super(Model, self).__init__()\n",
    "        ... # 根据传入的参数来定义子模块\n",
    "    \n",
    "    def forward(self, ...): # 定义前向计算的输入参数，...一般是张量或者其他的参数\n",
    "        ret = ... # 根据传入的张量和子模块计算返回张量\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, ndim):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.ndim = ndim\n",
    "\n",
    "        self.weight = nn.Parameter(torch.randn(ndim, 1)) # 定义权重\n",
    "        self.bias = nn.Parameter(torch.randn(1)) # 定义偏置\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 定义线性模型 y = Wx + b\n",
    "        return x.mm(self.weight) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4211],\n",
       "        [-2.8069],\n",
       "        [ 2.3589],\n",
       "        [-2.7430]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = LinearModel(5) # 定义线性回归模型，特征数为5\n",
    "x = torch.randn(4, 5) # 定义随机输入，迷你批次大小为4\n",
    "lm(x) # 得到每个迷你批次的输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LinearModel(5) # 定义线性模型\n",
    "x = torch.randn(4, 5) # 定义模型输入\n",
    "lm(x) # 根据模型获取输入对应的输出\n",
    "lm.named_parameters() # 获取模型参数（带名字）的生成器\n",
    "list(lm.named_parameters()) # 转换生成器为列表\n",
    "lm.parameters() # 获取模型参数（不带名字）的生成器\n",
    "list(lm.parameters()) # 转换生成器为列表\n",
    "lm.cuda() # 将模型参数移到GPU上\n",
    "list(lm.parameters()) # 显示模型参数，可以看到已经移到了GPU上（device='cuda:0'）\n",
    "lm.half() # 转换模型参数为半精度浮点数\n",
    "list(lm.parameters()) # 显示模型参数，可以看到已经转换为了半精度浮点数（dtype=torch.float16）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.randn(3, 3, requires_grad=True) # 定义一个3×3的张量\n",
    "t1\n",
    "t2 = t1.pow(2).sum() # 计算张量的所有分量平方和\n",
    "t2.backward() # 反向传播\n",
    "t1.grad # 梯度是张量原始分量的2倍\n",
    "t2 = t1.pow(2).sum() # 再次计算所有分量的平方和\n",
    "t2.backward() # 再次反向传播\n",
    "t1.grad # 梯度累积\n",
    "t1.grad.zero_() # 单个张量清零梯度的方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.randn(3, 3, requires_grad=True) # 初始化t1张量\n",
    "t2 = t1.sum()\n",
    "t2 # t2的计算构建了计算图，输出结果带有grad_fn\n",
    "with torch.no_grad():\n",
    "    t3 = t1.sum()\n",
    "t3 # t3的计算没有构建计算图，输出结果没有grad_fn\n",
    "t1.sum() # 保持原来的计算图\n",
    "t1.sum().detach() # 和原来的计算图分离"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.randn(3, 3, requires_grad=True) # 初始化t1张量\n",
    "t2 = t1.pow(2).sum() # 根据t1张量计算t2张量\n",
    "torch.autograd.grad(t2, t1) # t2张量对t1张量求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = nn.MSELoss() # 初始化平方损失函数模块\n",
    "t1 = torch.randn(5, requires_grad=True) # 随机生成张量t1\n",
    "t2 = torch.randn(5, requires_grad=True) # 随机生成张量t2\n",
    "mse(t1, t2) # 计算张量t1和t2之间的平方损失函数\n",
    "t1 = torch.randn(5, requires_grad=True) # 随机生成张量t1\n",
    "t1s = torch.sigmoid(t1)\n",
    "t2 = torch.randint(0, 2, (5, )).float() # 随机生成0，1的整数序列，并转换为浮点数\n",
    "# bce(t1s, t2) # 计算二分类的交叉熵\n",
    "bce_logits = nn.BCEWithLogitsLoss() # 使用交叉熵对数损失函数\n",
    "bce_logits(t1, t2) # 计算二分类的交叉熵，可以发现和前面的结果一致\n",
    "N=10 # 定义分类数目\n",
    "t1 = torch.randn(5, N, requires_grad=True) # 随机产生预测张量\n",
    "t2 = torch.randint(0, N, (5, )) # 随机产生目标张量\n",
    "t1s = torch.nn.functional.log_softmax(t1, -1) # 计算预测张量的LogSoftmax\n",
    "nll = nn.NLLLoss() # 定义NLL损失函数\n",
    "nll(t1s, t2) # 计算损失函数\n",
    "ce = nn.CrossEntropyLoss() # 定义交叉熵损失函数\n",
    "ce(t1, t2) # 计算损失函数，可以发现和NLL损失函数的结果一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "\n",
    "lm = LinearModel(13)\n",
    "criterion = nn.MSELoss()\n",
    "optim = torch.optim.SGD(lm.parameters(), lr=1e-6) # 定义优化器\n",
    "data = torch.tensor(boston[\"data\"], requires_grad=True, dtype=torch.float32)\n",
    "target = torch.tensor(boston[\"target\"], dtype=torch.float32)\n",
    "\n",
    "for step in range(10000):\n",
    "    predict = lm(data) # 输出模型预测结果\n",
    "    loss = criterion(predict, target) # 输出损失函数\n",
    "    if step and step % 1000 == 0 :\n",
    "        print(\"Loss: {:.3f}\".format(loss.item()))\n",
    "    optim.zero_grad() # 清零梯度\n",
    "    loss.backward() # 反向传播\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD([\n",
    "    {'params': model.base.parameters()},\n",
    "    {'params': model.classifier.parameters(), 'lr': 1e-3}\n",
    "], lr=1e-2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "for epoch in range(100):\n",
    "    train(...)\n",
    "    validate(...)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataLoader(dataset, batch_size=1, shuffle=False, sampler=None,\n",
    "            batch_sampler=None, num_workers=0, collate_fn=None,\n",
    "            pin_memory=False, drop_last=False, timeout=0,\n",
    "            worker_init_fn=None)\n",
    "# 该代码仅为演示函数签名所用，并不能实际运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 该代码仅为演示类的构造方法所用，并不能实际运行\n",
    "   （#号及其后面内容为注释，可以忽略）\n",
    "\"\"\"\n",
    "\n",
    "class Dataset(object):\n",
    "    def __getitem__(self, index):\n",
    "        # index: 数据缩索引（整数，范围为0到数据数目-1）\n",
    "        # ...\n",
    "        # 返回数据张量\n",
    "\n",
    "    def __len__(self):\n",
    "        # 返回数据的数目\n",
    "        # ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 该代码仅为演示类的构造方法所用，并不能实际运行\n",
    "\"\"\"\n",
    "\n",
    "class VisionDataset(data.Dataset):\n",
    "    def __init__(self, root, transforms=None, transform=None, \n",
    "        target_transform=None):\n",
    "        # ...\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def __len__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class DatasetFolder(VisionDataset):\n",
    "    def __init__(self, root, loader, extensions=None, transform=None,\n",
    "        target_transform=None, is_valid_file=None):\n",
    "        super(DatasetFolder, self).__init__(root, transform=transform,\n",
    "            target_transform=target_transform)\n",
    "        classes, class_to_idx = self._find_classes(self.root)\n",
    "        self.samples = make_dataset(self.root, class_to_idx, extensions, is_valid_file)\n",
    "        self.loader = loader\n",
    "        # ...\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.samples[index]\n",
    "        sample = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "        return sample, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 该代码仅为演示类的构造方法所用，并不能实际运行\n",
    "\"\"\"\n",
    "\n",
    "class MyIterableDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, start, end):\n",
    "        super(MyIterableDataset).__init__()\n",
    "            assert end > start, \\\n",
    "\"this example code only works with end >= start\"\n",
    "            self.start = start\n",
    "            self.end = end\n",
    "\n",
    "    def __iter__(self):\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        if worker_info is None:  # 单进程数据载入\n",
    "            iter_start = self.start\n",
    "            iter_end = self.end\n",
    "        else:  # 多进程，分割数据\n",
    "            per_worker = int(math.ceil((self.end - self.start) \\\n",
    "                            / float(worker_info.num_workers)))\n",
    "            worker_id = worker_info.id\n",
    "            iter_start = self.start + worker_id * per_worker\n",
    "            iter_end = min(iter_start + per_worker, self.end)\n",
    "        return iter(range(iter_start, iter_end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 该代码仅为演示函数签名所用，并不能实际运行\n",
    "\"\"\"\n",
    "\n",
    "torch.save(obj, f, pickle_module=pickle, pickle_protocol=2)\n",
    "torch.load(f, map_location=None, pickle_module=pickle, **pickle_load_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LinearModel(5) # 定义线性模型\n",
    "lm.state_dict() # 获取状态字典\n",
    "t = lm.state_dict() # 保存状态字典\n",
    "lm = LinearModel(5) # 重新定义线性模型\n",
    "lm.state_dict() # 新的状态字典，模型参数和原来的不同\n",
    "lm.load_state_dict(t) # 载入原来的状态字典\n",
    "lm.state_dict() # 模型参数已更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 该代码仅为演示函数签名所用，并不能实际运行\n",
    "\"\"\"\n",
    "\n",
    "save_info = { # 保存的信息\n",
    "    \"iter_num\": iter_num,  # 迭代步数 \n",
    "    \"optimizer\": optimizer.state_dict(), # 优化器的状态字典\n",
    "    \"model\": model.state_dict(), # 模型的状态字典\n",
    "}\n",
    "# 保存信息\n",
    "torch.save(save_info, save_path)\n",
    "# 载入信息\n",
    "save_info = torch.load(save_path)\n",
    "optimizer.load_state_dict(save_info[\"optimizer\"])\n",
    "model.load_state_dict(sae_info[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = load_boston()\n",
    "lm = LinearModel(13)\n",
    "criterion = nn.MSELoss()\n",
    "optim = torch.optim.SGD(lm.parameters(), lr=1e-6)\n",
    "data = torch.tensor(boston[\"data\"], requires_grad=True, dtype=torch.float32)\n",
    "target = torch.tensor(boston[\"target\"], dtype=torch.float32)\n",
    "writer = SummaryWriter() # 定义TensorBoard输出类\n",
    "for step in range(10000):\n",
    "    predict = lm(data)\n",
    "    loss = criterion(predict, target)\n",
    "    writer.add_scalar(\"Loss/train\", loss, step) # 输出损失函数\n",
    "    writer.add_histogram(\"Param/weight\", lm.weight, step) # 输出权重直方图\n",
    "    writer.add_histogram(\"Param/bias\", lm.bias, step) # 输出偏置直方图\n",
    "    if step and step % 1000 == 0 :\n",
    "        print(\"Loss: {:.3f}\".format(loss.item()))\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 该代码仅为演示函数签名所用，并不能实际运行\n",
    "\"\"\"\n",
    "\n",
    "torch.utils.tensorboard.writer.SummaryWriter(log_dir=None, comment='',\n",
    "    purge_step=None, max_queue=10, flush_secs=120, filename_suffix='')\n",
    "add_scalar(tag, scalar_value, global_step=None, walltime=None)\n",
    "add_scalars(main_tag, tag_scalar_dict, global_step=None, walltime=None)\n",
    "add_histogram(tag, values, global_step=None, bins='tensorflow', walltime=None, max_bins=None)\n",
    "add_image(tag, img_tensor, global_step=None, walltime=None,  dataformats='CHW')\n",
    "add_images(tag, img_tensor, global_step=None, walltime=None, dataformats='NCHW')\n",
    "add_figure(tag, figure, global_step=None, close=True, walltime=None)\n",
    "add_video(tag, vid_tensor, global_step=None, fps=4, walltime=None)\n",
    "add_audio(tag, snd_tensor, global_step=None, sample_rate=44100,\n",
    "    walltime=None)\n",
    "add_text(tag, text_string, global_step=None, walltime=None)\n",
    "add_graph(model, input_to_model=None, verbose=False)\n",
    "add_embedding(mat, metadata=None, label_img=None, global_step=None,\n",
    "    tag='default', metadata_header=None)\n",
    "add_pr_curve(tag, labels, predictions, global_step=None, num_thresholds=127,\n",
    "    weights=None, walltime=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 该代码仅为演示函数签名和所用方法，并不能实际运行\n",
    "\"\"\"\n",
    "\n",
    "torch.nn.DataParallel(module, device_ids=None, output_device=None, dim=0)\n",
    "model = … # 定义模型\n",
    "model = model.cuda()\n",
    "model = nn.DataParallel(model, device_ids=[0, 1, 2, 3]) # 数据并行\n",
    "output = model(input_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 该代码仅为演示函数签名所用，并不能实际运行\n",
    "\"\"\"\n",
    "\n",
    "torch.distributed.init_process_group(backend, init_method=None, \n",
    "    timeout=datetime.timedelta(0, 1800), world_size=-1, \n",
    "    rank=-1, store=None, group_name='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 该代码仅为演示函数签名所用，并不能实际运行\n",
    "\"\"\"\n",
    "\n",
    "train_dataset = datasets.ImageFolder(\n",
    "    traindir,\n",
    "    transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "train_sampler = \\\n",
    "    torch.utils.data.distributed.DistributedSampler(train_dataset)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=args.batch_size, \n",
    "    shuffle=(train_sampler is None),\n",
    "    num_workers=args.workers, pin_memory=True, sampler=train_sampler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 该代码仅为演示函数签名所用，并不能实际运行\n",
    "\"\"\"\n",
    "\n",
    "torch.nn.parallel.DistributedDataParallel(module, device_ids=None, \n",
    "    output_device=None, dim=0, broadcast_buffers=True, \n",
    "    process_group=None, bucket_cap_mb=25, \n",
    "    find_unused_parameters=False, check_reduction=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 该代码仅为演示函数签名所用，并不能实际运行\n",
    "\"\"\"\n",
    "\n",
    "if not args.multiprocessing_distributed or (args.multiprocessing_distributed\n",
    "    and args.rank % ngpus_per_node == 0):\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'arch': args.arch,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_acc1': best_acc1,\n",
    "        'optimizer' : optimizer.state_dict(),\n",
    "    }, is_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 该代码仅为演示函数签名所用，并不能实际运行\n",
    "\"\"\"\n",
    "\n",
    "class _ConvNd(in_channels, out_channels, kernel_size, stride,\n",
    "    padding, dilation, transposed, output_padding,\n",
    "    groups, bias, padding_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 该代码仅为演示函数签名所用，并不能实际运行\n",
    "\"\"\"\n",
    "\n",
    "# 批次归一化\n",
    "class torch.nn.BatchNorm1d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "class torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "class torch.nn.BatchNorm3d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "\n",
    "# 组归一化\n",
    "class torch.nn.GroupNorm(num_groups, num_channels, eps=1e-05, affine=True)\n",
    "\n",
    "# 实例归一化\n",
    "class torch.nn.InstanceNorm1d(num_features, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
    "class torch.nn.InstanceNorm2d(num_features, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
    "class torch.nn.InstanceNorm3d(num_features, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
    "\n",
    "# 层归一化\n",
    "class torch.nn.LocalResponseNorm(size, alpha=0.0001, beta=0.75, k=1.0)\n",
    "\n",
    "# 局部响应归一化\n",
    "class torch.nn.LocalResponseNorm(size, alpha=0.0001, beta=0.75, k=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 该代码仅为演示函数签名所用，并不能实际运行\n",
    "\"\"\"\n",
    "\n",
    "# 最大池化模块\n",
    "class torch.nn.MaxPool1d(kernel_size, stride=None, padding=0, dilation=1,return_indices=False, ceil_mode=False)\n",
    "class torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1,return_indices=False, ceil_mode=False)\n",
    "class torch.nn.MaxPool3d(kernel_size, stride=None, padding=0, dilation=1,return_indices=False, ceil_mode=False)\n",
    "\n",
    "# 平均池化模块\n",
    "class torch.nn.AvgPool1d(kernel_size, stride=None, padding=0,\n",
    "    ceil_mode=False, count_include_pad=True)\n",
    "class torch.nn.AvgPool2d(kernel_size, stride=None, padding=0,\n",
    "    ceil_mode=False, count_include_pad=True, divisor_override=None)\n",
    "class torch.nn.AvgPool3d(kernel_size, stride=None, padding=0,\n",
    "    ceil_mode=False, count_include_pad=True, divisor_override=None)\n",
    "\n",
    "# 乘幂平均池化模块\n",
    "class torch.nn.LPPool1d(norm_type, kernel_size, stride=None,\n",
    "    ceil_mode=False)\n",
    "class torch.nn.LPPool2d(norm_type, kernel_size, stride=None,\n",
    "    ceil_mode=False)\n",
    "\n",
    "# 分数最大池化模块\n",
    "class torch.nn.FractionalMaxPool2d(kernel_size, output_size=None,\n",
    "    output_ratio=None, return_indices=False, _random_samples=None)\n",
    "class torch.nn.FractionalMaxPool3d(kernel_size, output_size=None,\n",
    "    output_ratio=None, return_indices=False, _random_samples=None)\n",
    "\n",
    "# 自适应池化模块\n",
    "class torch.nn.AdaptiveMaxPool1d(output_size, return_indices=False)\n",
    "class torch.nn.AdaptiveMaxPool2d(output_size, return_indices=False)\n",
    "class torch.nn.AdaptiveMaxPool3d(output_size, return_indices=False)\n",
    "class torch.nn.AdaptiveAvgPool1d(output_size)\n",
    "class torch.nn.AdaptiveAvgPool2d(output_size)\n",
    "class torch.nn.AdaptiveAvgPool3d(output_size)\n",
    "\n",
    "# 最大反池化模块\n",
    "class torch.nn.MaxUnpool1d(kernel_size, stride=None, padding=0)\n",
    "class torch.nn.MaxUnpool2d(kernel_size, stride=None, padding=0)\n",
    "class torch.nn.MaxUnpool3d(kernel_size, stride=None, padding=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 该代码仅为演示函数签名所用，并不能实际运行\n",
    "\"\"\"\n",
    "\n",
    "class torch.nn.Dropout(p=0.5, inplace=False)\n",
    "class torch.nn.Dropout2d(p=0.5, inplace=False)\n",
    "class torch.nn.Dropout3d(p=0.5, inplace=False)\n",
    "class torch.nn.AlphaDropout(p=0.5, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 本文件中的代码可以通过使用命令 python ex_3_17.py 运行  \n",
    "   （#号及其后面内容为注释，可以忽略）\n",
    "\"\"\"\n",
    "# 情况1. 使用参数来构建顺序模型\n",
    "model = nn.Sequential(\n",
    "          nn.Conv2d(1,20,5),\n",
    "          nn.ReLU(),\n",
    "          nn.Conv2d(20,64,5),\n",
    "          nn.ReLU()\n",
    "        )\n",
    "\n",
    "print(model)\n",
    "\n",
    "# 情况2. 使用顺序字典来构建顺序模型\n",
    "model = nn.Sequential(OrderedDict([\n",
    "          ('conv1', nn.Conv2d(1,20,5)),\n",
    "          ('relu1', nn.ReLU()),\n",
    "          ('conv2', nn.Conv2d(20,64,5)),\n",
    "          ('relu2', nn.ReLU())\n",
    "        ]))\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 本文件中的代码可以通过使用命令 python ex_3_18.py 运行  \n",
    "   （#号及其后面内容为注释，可以忽略）\n",
    "\"\"\"\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "# 模块列表的使用方法\n",
    "class MyModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModule, self).__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(10)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 模块列表的迭代和使用方法与Python的普通列表一致\n",
    "        for i, l in enumerate(self.linears):\n",
    "            x = self.linears[i // 2](x) + l(x)\n",
    "        return x\n",
    "\n",
    "# 模块字典的使用方法\n",
    "class MyModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModule, self).__init__()\n",
    "        self.choices = nn.ModuleDict({\n",
    "                'conv': nn.Conv2d(10, 10, 3),\n",
    "                'pool': nn.MaxPool2d(3)\n",
    "        })\n",
    "        self.activations = nn.ModuleDict([\n",
    "                ['lrelu', nn.LeakyReLU()],\n",
    "                ['prelu', nn.PReLU()]\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, choice, act):\n",
    "        x = self.choices[choice](x)\n",
    "        x = self.activations[act](x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# 打印出模型的特征提取模块\n",
    "from torchvision.models import alexnet\n",
    "model = alexnet(pretrained=True)\n",
    "print(model.features)\n",
    "\n",
    "# 不同层特征提取模块的构造\n",
    "conv1 = nn.Sequential(*model.features[:1])\n",
    "conv2 = nn.Sequential(*model.features[:4])\n",
    "conv3_1 = nn.Sequential(*model.features[:7])\n",
    "conv3_2 = nn.Sequential(*model.features[:9])\n",
    "conv3_3 = nn.Sequential(*model.features[:11])\n",
    "\n",
    "# 根据输入的图像张量（1×3×224×224）输出特征张量\n",
    "feat1 = conv1(img)\n",
    "feat2 = conv2(img)\n",
    "feat3_1 = conv3_1(img)\n",
    "feat3_2 = conv3_2(img)\n",
    "feat3_3 = conv3_3(img)\n",
    "print(feat1.shape)\n",
    "print(feat2.shape)\n",
    "print(feat3_1.shape)\n",
    "print(feat3_2.shape)\n",
    "print(feat3_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 该代码仅为演示函数签名所用，并不能实际运行\n",
    "\"\"\"\n",
    "# 计算增益系数函数签名\n",
    "torch.nn.init.calculate_gain(nonlinearity, param=None)\n",
    "\n",
    "# 计算并且打印增益系数\n",
    "gain = nn.init.calculate_gain('leaky_relu', 0.2)\n",
    "print(gain)\n",
    "\n",
    "# 参数初始化函数签名\n",
    "torch.nn.init.uniform_(tensor, a=0.0, b=1.0)\n",
    "torch.nn.init.normal_(tensor, mean=0.0, std=1.0)\n",
    "torch.nn.init.ones_(tensor)\n",
    "torch.nn.init.zeros_(tensor)\n",
    "torch.nn.init.xavier_uniform_(tensor, gain=1.0)\n",
    "torch.nn.init.xavier_normal_(tensor, gain=1.0)\n",
    "torch.nn.init.kaiming_uniform_(tensor, a=0, mode='fan_in',\n",
    "    nonlinearity='leaky_relu')\n",
    "torch.nn.init.kaiming_normal_(tensor, a=0, mode='fan_in',\n",
    "    nonlinearity='leaky_relu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 该代码为搭建模型代码,可以被其它脚本的导入\n",
    "\"\"\"\n",
    "\n",
    "class BasicConv2d(nn.Module):\n",
    "\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, \n",
    "                 stride, padding=0):\n",
    "        super(BasicConv2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes,\n",
    "                              kernel_size=kernel_size, stride=stride,\n",
    "                              padding=padding, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_planes,\n",
    "                                 eps=0.001, \n",
    "                                 momentum=0.1,\n",
    "                                 affine=True)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class Inception_A(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Inception_A, self).__init__()\n",
    "        self.branch0 = BasicConv2d(384, 96, kernel_size=1, stride=1)\n",
    "\n",
    "        self.branch1 = nn.Sequential(\n",
    "            BasicConv2d(384, 64, kernel_size=1, stride=1),\n",
    "            BasicConv2d(64, 96, kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "\n",
    "        self.branch2 = nn.Sequential(\n",
    "            BasicConv2d(384, 64, kernel_size=1, stride=1),\n",
    "            BasicConv2d(64, 96, kernel_size=3, stride=1, padding=1),\n",
    "            BasicConv2d(96, 96, kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "\n",
    "        self.branch3 = nn.Sequential(\n",
    "            nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False),\n",
    "            BasicConv2d(384, 96, kernel_size=1, stride=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.branch0(x)\n",
    "        x1 = self.branch1(x)\n",
    "        x2 = self.branch2(x)\n",
    "        x3 = self.branch3(x)\n",
    "        out = torch.cat((x0, x1, x2, x3), 1)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 该代码为搭建模型代码,可以被其它脚本的导入\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 深度残差网络基础代码\n",
    "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=dilation, groups=groups, \n",
    "bias=False, dilation=dilation)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    __constants__ = ['downsample']\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "# 深度分离卷积模块代码\n",
    "class DepthWiseSep(nn.Module):\n",
    "    def __init__(self, nin, kernels_per_layer, nout):\n",
    "        super(DepthWiseSep, self).__init__()\n",
    "        self.depthwise = nn.Conv2d(nin, nin * kernels_per_layer, kernel_size=3, padding=1, groups=nin)\n",
    "        self.pointwise = nn.Conv2d(nin * kernels_per_layer, nout, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.depthwise(x)\n",
    "        out = self.pointwise(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 该代码用于LeNet的训练数据集MNIST的载入\n",
    "\"\"\"\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data_train = MNIST('./data',\n",
    "                   download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.Resize((32, 32)),\n",
    "                       transforms.ToTensor()]))\n",
    "data_test = MNIST('./data',\n",
    "                  train=False,\n",
    "                  download=True,\n",
    "                  transform=transforms.Compose([\n",
    "                      transforms.Resize((32, 32)),\n",
    "                      transforms.ToTensor()]))\n",
    "data_train_loader = DataLoader(data_train, batch_size=256,\n",
    "    shuffle=True, num_workers=8)\n",
    "data_test_loader = DataLoader(data_test, batch_size=1024,\n",
    "    num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import LeNet\n",
    "\n",
    "# ... 此处略去定义测试数据载入器的代码，具体参考代码4.3\n",
    "\n",
    "# save_info = { # 保存的信息\n",
    "#    \"iter_num\": iter_num,  # 迭代步数 \n",
    "#    \"optimizer\": optimizer.state_dict(), # 优化器的状态字典\n",
    "#    \"model\": model.state_dict(), # 模型的状态字典\n",
    "# }\n",
    " \n",
    "model_path = \"./model.pth\" # 假设模型保存在model.pth文件中\n",
    "save_info = torch.load(model_path) # 载入模型\n",
    "model = LeNet() # 定义LeNet模型\n",
    "criterion = nn.CrossEntropyLoss() # 定义损失函数\n",
    "model.load_state_dict(save_info[\"model\"]) # 载入模型参数\n",
    "model.eval() # 切换模型到测试状态\n",
    "\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad(): # 关闭计算图\n",
    "    for batch_idx, (inputs, targets) in enumerate(data_test_loader):\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        print(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "            % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 该代码定义了LeNet模型\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.fc3 = nn.Linear(16*6*6, 120)\n",
    "        self.fc4 = nn.Linear(120, 84)\n",
    "        self.fc5 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(torch.relu(self.conv1(x)))\n",
    "        x = self.pool2(torch.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = LeNet() # 初始化实例\n",
    "    ret = model(torch.randn(1, 1, 32, 32)) # 输入一张图片，测试输出结果\n",
    "    ret.shape # torch.Size([1, 10])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 该代码定义了一些工程使用的工具脚本\n",
    "\"\"\"\n",
    "\n",
    "def plot_mnist():\n",
    "    import matplotlib.pyplot as plt\n",
    "    figure = plt.figure()\n",
    "    num_of_images = 60\n",
    "\n",
    "    for imgs, targets in data_train_loader:\n",
    "        break\n",
    "\n",
    "    for index in range(num_of_images):\n",
    "        plt.subplot(6, 10, index+1)\n",
    "        plt.axis('off')\n",
    "        img = imgs[index, ...]\n",
    "        plt.imshow(img.numpy().squeeze(), cmap='gray_r')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 该代码仅为演示函数签名所用，并不能实际运行\n",
    "\"\"\"\n",
    "\n",
    "class torchvision.datasets.MNIST(root, train=True, transform=None,\n",
    "    target_transform=None, download=False)\n",
    "class torchvision.datasets.CIFAR10(root, train=True, transform=None,\n",
    "    target_transform=None, download=False)\n",
    "class torchvision.datasets.VOCSegmentation(root, year='2012',\n",
    "    image_set='train'， download=False, transform=None, \n",
    "    target_transform=None, transforms=None)\n",
    "class torchvision.datasets.ImageNet(root, split='train', download=False,\n",
    "    **kwargs)\n",
    "class torchvision.datasets.CocoDetection(root, annFile, transform=None, \n",
    "    target_transform=None, transforms=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch LeNet Training')\n",
    "parser.add_argument('--lr', default=0.01, type=float, help='Learning rate')\n",
    "parser.add_argument('--batch-size', '-b', default=256, type=int,\n",
    "    help='Batchsize')\n",
    "args = parser.parse_args()\n",
    "\n",
    "print(args.lr, args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 本代码为示例代码，演示如何载入ImageNet的图像数据集\n",
    "\"\"\"\n",
    "\n",
    "def main_worker(gpu, ngpus_per_node, args):\n",
    "    # ...\n",
    "    # 设置数据集目录\n",
    "    traindir = os.path.join(args.data, 'train')\n",
    "    valdir = os.path.join(args.data, 'val')\n",
    "    # 设置预处理方法\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    # 训练数据集\n",
    "    train_dataset = datasets.ImageFolder(\n",
    "        traindir,\n",
    "        transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]))\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=args.batch_size, \n",
    "        shuffle=(train_sampler is None),\n",
    "        num_workers=args.workers, pin_memory=True, \n",
    "        sampler=train_sampler)\n",
    "\n",
    "    # 测试数据集\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        datasets.ImageFolder(valdir, transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])),\n",
    "        batch_size=args.batch_size, shuffle=False,\n",
    "        num_workers=args.workers, pin_memory=True)\n",
    "    # ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 本代码可以被其它代码导入，作为模型的一部分\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        width = int(planes * (base_width / 64.)) * groups\n",
    "        \n",
    "        self.conv1 = conv1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 本代码可以被其它代码导入，作为模型的一部分\n",
    "\"\"\"\n",
    "\n",
    "# InceptionA 模块\n",
    "class InceptionA(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, pool_features):\n",
    "        super(InceptionA, self).__init__()\n",
    "        self.branch1x1 = BasicConv2d(in_channels, 64, kernel_size=1)\n",
    "\n",
    "        self.branch5x5_1 = BasicConv2d(in_channels, 48, kernel_size=1)\n",
    "        self.branch5x5_2 = BasicConv2d(48, 64, kernel_size=5, padding=2)\n",
    "\n",
    "        self.branch3x3dbl_1 = BasicConv2d(in_channels, 64, kernel_size=1)\n",
    "        self.branch3x3dbl_2 = BasicConv2d(64, 96, kernel_size=3, padding=1)\n",
    "        self.branch3x3dbl_3 = BasicConv2d(96, 96, kernel_size=3, padding=1)\n",
    "\n",
    "        self.branch_pool = BasicConv2d(in_channels, pool_features,\n",
    "            kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "\n",
    "        branch5x5 = self.branch5x5_1(x)\n",
    "        branch5x5 = self.branch5x5_2(branch5x5)\n",
    "\n",
    "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
    "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
    "        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n",
    "\n",
    "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
    "        branch_pool = self.branch_pool(branch_pool)\n",
    "\n",
    "        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "# InceptionB 模块\n",
    "class InceptionB(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels):\n",
    "        super(InceptionB, self).__init__()\n",
    "        self.branch3x3 = BasicConv2d(in_channels, 384, kernel_size=3,\n",
    "            stride=2)\n",
    "\n",
    "        self.branch3x3dbl_1 = BasicConv2d(in_channels, 64, kernel_size=1)\n",
    "        self.branch3x3dbl_2 = BasicConv2d(64, 96, kernel_size=3, padding=1)\n",
    "        self.branch3x3dbl_3 = BasicConv2d(96, 96, kernel_size=3, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch3x3 = self.branch3x3(x)\n",
    "\n",
    "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
    "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
    "        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n",
    "\n",
    "        branch_pool = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "\n",
    "        outputs = [branch3x3, branch3x3dbl, branch_pool]\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "# InceptionC 模块\n",
    "class InceptionC(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, channels_7x7):\n",
    "        super(InceptionC, self).__init__()\n",
    "        self.branch1x1 = BasicConv2d(in_channels, 192, kernel_size=1)\n",
    "\n",
    "        c7 = channels_7x7\n",
    "        self.branch7x7_1 = BasicConv2d(in_channels, c7, kernel_size=1)\n",
    "        self.branch7x7_2 = BasicConv2d(c7, c7, kernel_size=(1, 7),\n",
    "            padding=(0, 3))\n",
    "        self.branch7x7_3 = BasicConv2d(c7, 192, kernel_size=(7, 1),\n",
    "            padding=(3, 0))\n",
    "\n",
    "        self.branch7x7dbl_1 = BasicConv2d(in_channels, c7, kernel_size=1)\n",
    "        self.branch7x7dbl_2 = BasicConv2d(c7, c7, kernel_size=(7, 1),\n",
    "            padding=(3, 0))\n",
    "        self.branch7x7dbl_3 = BasicConv2d(c7, c7, kernel_size=(1, 7),\n",
    "            padding=(0, 3))\n",
    "        self.branch7x7dbl_4 = BasicConv2d(c7, c7, kernel_size=(7, 1),\n",
    "            padding=(3, 0))\n",
    "        self.branch7x7dbl_5 = BasicConv2d(c7, 192, kernel_size=(1, 7),\n",
    "            padding=(0, 3))\n",
    "\n",
    "        self.branch_pool = BasicConv2d(in_channels, 192, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "\n",
    "        branch7x7 = self.branch7x7_1(x)\n",
    "        branch7x7 = self.branch7x7_2(branch7x7)\n",
    "        branch7x7 = self.branch7x7_3(branch7x7)\n",
    "\n",
    "        branch7x7dbl = self.branch7x7dbl_1(x)\n",
    "        branch7x7dbl = self.branch7x7dbl_2(branch7x7dbl)\n",
    "        branch7x7dbl = self.branch7x7dbl_3(branch7x7dbl)\n",
    "        branch7x7dbl = self.branch7x7dbl_4(branch7x7dbl)\n",
    "        branch7x7dbl = self.branch7x7dbl_5(branch7x7dbl)\n",
    "\n",
    "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
    "        branch_pool = self.branch_pool(branch_pool)\n",
    "\n",
    "        outputs = [branch1x1, branch7x7, branch7x7dbl, branch_pool]\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "# InceptionD 模块\n",
    "class InceptionD(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels):\n",
    "        super(InceptionD, self).__init__()\n",
    "        self.branch3x3_1 = BasicConv2d(in_channels, 192, kernel_size=1)\n",
    "        self.branch3x3_2 = BasicConv2d(192, 320, kernel_size=3, stride=2)\n",
    "\n",
    "        self.branch7x7x3_1 = BasicConv2d(in_channels, 192, kernel_size=1)\n",
    "        self.branch7x7x3_2 = BasicConv2d(192, 192, kernel_size=(1, 7), padding=(0, 3))\n",
    "        self.branch7x7x3_3 = BasicConv2d(192, 192, kernel_size=(7, 1), padding=(3, 0))\n",
    "        self.branch7x7x3_4 = BasicConv2d(192, 192, kernel_size=3, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch3x3 = self.branch3x3_1(x)\n",
    "        branch3x3 = self.branch3x3_2(branch3x3)\n",
    "\n",
    "        branch7x7x3 = self.branch7x7x3_1(x)\n",
    "        branch7x7x3 = self.branch7x7x3_2(branch7x7x3)\n",
    "        branch7x7x3 = self.branch7x7x3_3(branch7x7x3)\n",
    "        branch7x7x3 = self.branch7x7x3_4(branch7x7x3)\n",
    "\n",
    "        branch_pool = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "        outputs = [branch3x3, branch7x7x3, branch_pool]\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "# InceptionE 模块\n",
    "class InceptionE(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels):\n",
    "        super(InceptionE, self).__init__()\n",
    "        self.branch1x1 = BasicConv2d(in_channels, 320, kernel_size=1)\n",
    "\n",
    "        self.branch3x3_1 = BasicConv2d(in_channels, 384, kernel_size=1)\n",
    "        self.branch3x3_2a = BasicConv2d(384, 384, kernel_size=(1, 3),\n",
    "            padding=(0, 1))\n",
    "        self.branch3x3_2b = BasicConv2d(384, 384, kernel_size=(3, 1),\n",
    "            padding=(1, 0))\n",
    "\n",
    "        self.branch3x3dbl_1 = BasicConv2d(in_channels, 448, kernel_size=1)\n",
    "        self.branch3x3dbl_2 = BasicConv2d(448, 384, kernel_size=3, padding=1)\n",
    "        self.branch3x3dbl_3a = BasicConv2d(384, 384, kernel_size=(1, 3),\n",
    "            padding=(0, 1))\n",
    "        self.branch3x3dbl_3b = BasicConv2d(384, 384, kernel_size=(3, 1),\n",
    "            padding=(1, 0))\n",
    "\n",
    "        self.branch_pool = BasicConv2d(in_channels, 192, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "\n",
    "        branch3x3 = self.branch3x3_1(x)\n",
    "        branch3x3 = [\n",
    "            self.branch3x3_2a(branch3x3),\n",
    "            self.branch3x3_2b(branch3x3),\n",
    "        ]\n",
    "        branch3x3 = torch.cat(branch3x3, 1)\n",
    "\n",
    "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
    "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
    "        branch3x3dbl = [\n",
    "            self.branch3x3dbl_3a(branch3x3dbl),\n",
    "            self.branch3x3dbl_3b(branch3x3dbl),\n",
    "        ]\n",
    "        branch3x3dbl = torch.cat(branch3x3dbl, 1)\n",
    "\n",
    "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
    "        branch_pool = self.branch_pool(branch_pool)\n",
    "\n",
    "        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "# InceptionAux 模块\n",
    "class InceptionAux(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(InceptionAux, self).__init__()\n",
    "        self.conv0 = BasicConv2d(in_channels, 128, kernel_size=1)\n",
    "        self.conv1 = BasicConv2d(128, 768, kernel_size=5)\n",
    "        self.conv1.stddev = 0.01\n",
    "        self.fc = nn.Linear(768, num_classes)\n",
    "        self.fc.stddev = 0.001\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.avg_pool2d(x, kernel_size=5, stride=3)\n",
    "        x = self.conv0(x)\n",
    "        x = self.conv1(x)\n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 该代码改编自PyTorch官网torchvision源代码 \n",
    "    https://github.com/pytorch/vision/blob/master/torchvision/models/segmentation/fcn.py\n",
    "\"\"\"\n",
    "\n",
    "# FCN特征提取部分\n",
    "def _segm_resnet(name, backbone_name, num_classes, aux,\n",
    "    pretrained_backbone=True):\n",
    "    \n",
    "    backbone = resnet.__dict__[backbone_name](\n",
    "        pretrained=pretrained_backbone,\n",
    "        replace_stride_with_dilation=[False, True, True])\n",
    "\n",
    "    return_layers = {'layer4': 'out'}\n",
    "    if aux:\n",
    "        return_layers['layer3'] = 'aux'\n",
    "    backbone = IntermediateLayerGetter(backbone, return_layers=return_layers)\n",
    "\n",
    "    aux_classifier = None\n",
    "    if aux:\n",
    "        inplanes = 1024\n",
    "        aux_classifier = FCNHead(inplanes, num_classes)\n",
    "\n",
    "    model_map = {\n",
    "        'deeplabv3': (DeepLabHead, DeepLabV3),\n",
    "        'fcn': (FCNHead, FCN),\n",
    "    }\n",
    "    inplanes = 2048\n",
    "    classifier = model_map[name][0](inplanes, num_classes)\n",
    "    base_model = model_map[name][1]\n",
    "\n",
    "    model = base_model(backbone, classifier, aux_classifier)\n",
    "    return model\n",
    "\n",
    "# FCN模块部分\n",
    "class FCN(_SimpleSegmentationModel):\n",
    "    pass\n",
    "\n",
    "class _SimpleSegmentationModel(nn.Module):\n",
    "    def __init__(self, backbone, classifier, aux_classifier=None):\n",
    "        super(_SimpleSegmentationModel, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.classifier = classifier\n",
    "        self.aux_classifier = aux_classifier\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_shape = x.shape[-2:]\n",
    "        features = self.backbone(x)\n",
    "\n",
    "        result = OrderedDict()\n",
    "        x = features[\"out\"]\n",
    "        x = self.classifier(x)\n",
    "        x = F.interpolate(x, size=input_shape, mode='bilinear',\n",
    "            align_corners=False)\n",
    "        result[\"out\"] = x\n",
    "\n",
    "        if self.aux_classifier is not None:\n",
    "            x = features[\"aux\"]\n",
    "            x = self.aux_classifier(x)\n",
    "            x = F.interpolate(x, size=input_shape, mode='bilinear',\n",
    "                align_corners=False)\n",
    "            result[\"aux\"] = x\n",
    "\n",
    "        return result\n",
    "\n",
    "# FCNHead部分\n",
    "class FCNHead(nn.Sequential):\n",
    "    def __init__(self, in_channels, channels):\n",
    "        inter_channels = in_channels // 4\n",
    "        layers = [\n",
    "            nn.Conv2d(in_channels, inter_channels, 3, padding=1,\n",
    "                bias=False),\n",
    "            nn.BatchNorm2d(inter_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Conv2d(inter_channels, channels, 1)\n",
    "        ]\n",
    "\n",
    "        super(FCNHead, self).__init__(*layers)\n",
    "\n",
    "# FCN输入图像预处理\n",
    "def get_transform(train):\n",
    "    base_size = 520\n",
    "    crop_size = 480\n",
    "\n",
    "    min_size = int((0.5 if train else 1.0) * base_size)\n",
    "    max_size = int((2.0 if train else 1.0) * base_size)\n",
    "    transforms = []\n",
    "    transforms.append(T.RandomResize(min_size, max_size))\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "        transforms.append(T.RandomCrop(crop_size))\n",
    "    transforms.append(T.ToTensor())\n",
    "    transforms.append(T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                  std=[0.229, 0.224, 0.225]))\n",
    "\n",
    "    return T.Compose(transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 以下代码仅作为DC-GAN模型的实现参考\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "#  生成器的定义\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "\n",
    "            nn.ConvTranspose2d(nz, ngf*8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(ngf*8, ngf*4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(ngf*4, ngf*2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf*2),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(ngf*2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output\n",
    "\n",
    "# 判别器的定义\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        output = self.main(input)\n",
    "        return output.view(-1, 1).squeeze(1)\n",
    "\n",
    "# DCGAN的训练代码\n",
    "\n",
    "def train():\n",
    "    netG = Generator(ngpu).to(device)\n",
    "    netD = Discriminator(ngpu).to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizerD = optim.Adam(netD.parameters(), lr=opt.lr,\n",
    "        betas=(opt.beta1, 0.999))\n",
    "    optimizerG = optim.Adam(netG.parameters(), lr=opt.lr,\n",
    "        betas=(opt.beta1, 0.999))\n",
    "    real_label = 1 # 真实图像标签\n",
    "    fake_label = 0 # 生成图像标签\n",
    "    for epoch in range(opt.niter):\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            # 判别器梯度置为0\n",
    "            netD.zero_grad()\n",
    "            real_cpu = data[0].to(device)\n",
    "            batch_size = real_cpu.size(0)\n",
    "            # 定义输入数据\n",
    "            label = torch.full((batch_size,), real_label, device=device)\n",
    "            output = netD(real_cpu)\n",
    "            # 定义判别器相对于真实图像损失函数\n",
    "            errD_real = criterion(output, label)\n",
    "            # 梯度反向传播，相对于真实图像\n",
    "            errD_real.backward()\n",
    "            noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "            fake = netG(noise)\n",
    "            label.fill_(fake_label)\n",
    "            output = netD(fake.detach())\n",
    "            # 定义判别器相对于生成图像损失函数\n",
    "            errD_fake = criterion(output, label)\n",
    "            # 梯度反向传播，相对于生成图像\n",
    "            errD_fake.backward()\n",
    "            # 计算判别器总的损失函数：真实图像损失函数+生成图像损失函数\n",
    "            errD = errD_real + errD_fake\n",
    "            # 优化判别器\n",
    "            optimizerD.step()        \n",
    "    \n",
    "            # 生成器梯度置为0\n",
    "            netG.zero_grad()\n",
    "            # 注意这里是real_label，相对于前面fake_label\n",
    "            label.fill_(real_label)\n",
    "            output = netD(fake)\n",
    "            # 定义判别器相对于真实图像损失函数\n",
    "            errG = criterion(output, label)\n",
    "            # 梯度反向传播，相对于生成器\n",
    "            errG.backward()\n",
    "            # 优化生成器\n",
    "            optimizerG.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 该代码改编自PyTorch的教程：\n",
    "    https://pytorch.org/tutorials/advanced/neural_style_tutorial.html\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# 内容损失函数\n",
    "class ContentLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, target,):\n",
    "        super(ContentLoss, self).__init__()\n",
    "        self.target = target.detach()\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.loss = F.mse_loss(input, self.target)\n",
    "        return input\n",
    "\n",
    "# 风格损失函数\n",
    "def gram_matrix(input):\n",
    "\n",
    "    a, b, c, d = input.size()\n",
    "    features = input.view(a * b, c * d)\n",
    "    G = torch.mm(features, features.t()) \n",
    "\n",
    "    return G.div(a * b * c * d)\n",
    "\n",
    "class StyleLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, target_feature):\n",
    "        super(StyleLoss, self).__init__()\n",
    "        self.target = gram_matrix(target_feature).detach()\n",
    "\n",
    "    def forward(self, input):\n",
    "        G = gram_matrix(input)\n",
    "        self.loss = F.mse_loss(G, self.target)\n",
    "        return input\n",
    "\n",
    "# 图像归一化的变换模块\n",
    "class Normalization(nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        super(Normalization, self).__init__()\n",
    "        self.mean = torch.tensor(mean).view(-1, 1, 1)\n",
    "        self.std = torch.tensor(std).view(-1, 1, 1)\n",
    "\n",
    "    def forward(self, img):\n",
    "        return (img - self.mean) / self.std\n",
    "\n",
    "# 特征的提取和损失函数的计算\n",
    "content_layers_default = ['conv_4']\n",
    "style_layers_default = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n",
    "\n",
    "def get_style_model_and_losses(cnn, normalization_mean, normalization_std,\n",
    "                               style_img, content_img,\n",
    "                               content_layers=content_layers_default,\n",
    "                               style_layers=style_layers_default):\n",
    "    cnn = copy.deepcopy(cnn)\n",
    "\n",
    "    normalization = Normalization(normalization_mean,\n",
    "        normalization_std).to(device)\n",
    "\n",
    "    content_losses = []\n",
    "    style_losses = []\n",
    "\n",
    "    model = nn.Sequential(normalization)\n",
    "\n",
    "    i = 0  # increment every time we see a conv\n",
    "    for layer in cnn.children():\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            i += 1\n",
    "            name = 'conv_{}'.format(i)\n",
    "        elif isinstance(layer, nn.ReLU):\n",
    "            name = 'relu_{}'.format(i)\n",
    "            layer = nn.ReLU(inplace=False)\n",
    "        elif isinstance(layer, nn.MaxPool2d):\n",
    "            name = 'pool_{}'.format(i)\n",
    "        elif isinstance(layer, nn.BatchNorm2d):\n",
    "            name = 'bn_{}'.format(i)\n",
    "        else:\n",
    "            raise RuntimeError('Unrecognized layer: {}'\\\n",
    "                .format(layer.__class__.__name__))\n",
    "\n",
    "        model.add_module(name, layer)\n",
    "\n",
    "        if name in content_layers:\n",
    "            target = model(content_img).detach()\n",
    "            content_loss = ContentLoss(target)\n",
    "            model.add_module(\"content_loss_{}\".format(i), content_loss)\n",
    "            content_losses.append(content_loss)\n",
    "\n",
    "        if name in style_layers:\n",
    "            target_feature = model(style_img).detach()\n",
    "            style_loss = StyleLoss(target_feature)\n",
    "            model.add_module(\"style_loss_{}\".format(i), style_loss)\n",
    "            style_losses.append(style_loss)\n",
    "\n",
    "    for i in range(len(model) - 1, -1, -1):\n",
    "        if isinstance(model[i], ContentLoss) or \\\n",
    "            isinstance(model[i], StyleLoss):\n",
    "            break\n",
    "\n",
    "    model = model[:(i + 1)]\n",
    "\n",
    "    return model, style_losses, content_losses\n",
    "\n",
    "# 输出图像的初始化和优化器的定义\n",
    "input_img = content_img.clone()\n",
    "# 如果需要对风格迁移输出张量进行随机初始化，使用以下代码\n",
    "# input_img = torch.randn(content_img.data.size(), device=device)\n",
    "def get_input_optimizer(input_img):\n",
    "    optimizer = optim.LBFGS([input_img.requires_grad_()])\n",
    "    return optimizer\n",
    "\n",
    "# 输出图像的优化算法\n",
    "def run_style_transfer(cnn, normalization_mean, normalization_std,\n",
    "                       content_img, style_img, input_img, num_steps=300,\n",
    "                       style_weight=1000000, content_weight=1):\n",
    "\n",
    "    model, style_losses, content_losses = get_style_model_and_losses(cnn,\n",
    "        normalization_mean, normalization_std, style_img, content_img)\n",
    "    optimizer = get_input_optimizer(input_img)\n",
    "\n",
    "    run = [0]\n",
    "    while run[0] <= num_steps:\n",
    "\n",
    "        def closure():\n",
    "\n",
    "            input_img.data.clamp_(0, 1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            model(input_img)\n",
    "            style_score = 0\n",
    "            content_score = 0\n",
    "\n",
    "            for sl in style_losses:\n",
    "                style_score += sl.loss\n",
    "            for cl in content_losses:\n",
    "                content_score += cl.loss\n",
    "\n",
    "            style_score *= style_weight\n",
    "            content_score *= content_weight\n",
    "\n",
    "            loss = style_score + content_score\n",
    "            loss.backward()\n",
    "\n",
    "            run[0] += 1\n",
    "            if run[0] % 50 == 0:\n",
    "                print(\"run {}:\".format(run))\n",
    "                print('Style Loss : {:4f} Content Loss: {:4f}'.format(\n",
    "                    style_score.item(), content_score.item()))\n",
    "                print()\n",
    "\n",
    "            return style_score + content_score\n",
    "\n",
    "        optimizer.step(closure)\n",
    "\n",
    "    input_img.data.clamp_(0, 1)\n",
    "\n",
    "    return input_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 该代码改编自以下Repo：\n",
    "    https://github.com/mateuszbuda/brain-segmentation-pytorch\n",
    "\"\"\"\n",
    "\n",
    "class UNet(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels=3, out_channels=1, init_features=64):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        features = init_features\n",
    "        self.encoder1 = UNet._block(in_channels, features, name=\"enc1\")\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder2 = UNet._block(features, features * 2, name=\"enc2\")\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder3 = UNet._block(features * 2, features * 4, name=\"enc3\")\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder4 = UNet._block(features * 4, features * 8, name=\"enc4\")\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.bottleneck = UNet._block(features * 8, features * 16,\n",
    "            name=\"bottleneck\")\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(\n",
    "            features * 16, features * 8, kernel_size=2, stride=2)\n",
    "        self.decoder4 = UNet._block((features * 8) * 2, features * 8,\n",
    "            name=\"dec4\")\n",
    "        self.upconv3 = nn.ConvTranspose2d(\n",
    "            features * 8, features * 4, kernel_size=2, stride=2)\n",
    "        self.decoder3 = UNet._block((features * 4) * 2, features * 4,\n",
    "            name=\"dec3\")\n",
    "        self.upconv2 = nn.ConvTranspose2d(\n",
    "            features * 4, features * 2, kernel_size=2, stride=2)\n",
    "        self.decoder2 = UNet._block((features * 2) * 2, features * 2,\n",
    "            name=\"dec2\")\n",
    "        self.upconv1 = nn.ConvTranspose2d(\n",
    "            features * 2, features, kernel_size=2, stride=2)\n",
    "        self.decoder1 = UNet._block(features * 2, features, name=\"dec1\")\n",
    "\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=features, out_channels=out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(self.pool1(enc1))\n",
    "        enc3 = self.encoder3(self.pool2(enc2))\n",
    "        enc4 = self.encoder4(self.pool3(enc3))\n",
    "\n",
    "        bottleneck = self.bottleneck(self.pool4(enc4))\n",
    "\n",
    "        dec4 = self.upconv4(bottleneck)\n",
    "        dec4 = torch.cat((dec4, enc4), dim=1)\n",
    "        dec4 = self.decoder4(dec4)\n",
    "        dec3 = self.upconv3(dec4)\n",
    "        dec3 = torch.cat((dec3, enc3), dim=1)\n",
    "        dec3 = self.decoder3(dec3)\n",
    "        dec2 = self.upconv2(dec3)\n",
    "        dec2 = torch.cat((dec2, enc2), dim=1)\n",
    "        dec2 = self.decoder2(dec2)\n",
    "        dec1 = self.upconv1(dec2)\n",
    "        dec1 = torch.cat((dec1, enc1), dim=1)\n",
    "        dec1 = self.decoder1(dec1)\n",
    "        return torch.sigmoid(self.conv(dec1))\n",
    "\n",
    "    @staticmethod\n",
    "    def _block(in_channels, features, name):\n",
    "        return nn.Sequential(\n",
    "                    nn.Conv2d(in_channels=in_channels, out_channels=features,\n",
    "                        kernel_size=3, padding=1, bias=False,),\n",
    "                    nn.BatchNorm2d(num_features=features),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Conv2d(in_channels=features, out_channels=features,\n",
    "                        kernel_size=3, padding=1, bias=False,),\n",
    "                    nn.BatchNorm2d(num_features=features),\n",
    "                    nn.ReLU(inplace=True),)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 以下代码仅作为模型构建的演示代码\n",
    "\"\"\"\n",
    "\n",
    "# VAE模型的定义\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(784, 400)\n",
    "        self.fc21 = nn.Linear(400, 20)\n",
    "        self.fc22 = nn.Linear(400, 20)\n",
    "        self.fc3 = nn.Linear(20, 400)\n",
    "        self.fc4 = nn.Linear(400, 784)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "# VAE模型的损失函数\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自然语言处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x9 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 19 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',]\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 1, 0, 1],\n",
       "       [0, 1, 0, 1, 0, 2, 1, 0, 1],\n",
       "       [1, 0, 0, 0, 1, 0, 1, 1, 0],\n",
       "       [0, 1, 1, 1, 0, 0, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 8,\n",
       " 'is': 3,\n",
       " 'the': 6,\n",
       " 'first': 2,\n",
       " 'document': 1,\n",
       " 'second': 5,\n",
       " 'and': 0,\n",
       " 'third': 7,\n",
       " 'one': 4}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 该代码仅为演示函数签名和所用方法，并不能实际运行\n",
    "\"\"\"\n",
    "\n",
    "class sklearn.feature_extraction.text.CountVectorizer(input='content',\n",
    "    encoding='utf-8', decode_error='strict', strip_accents=None, \n",
    "    lowercase=True, preprocessor=None, tokenizer=None,\n",
    "    stop_words=None, token_pattern='(?u)\\b\\w\\w+\\b', \n",
    "    ngram_range=(1, 1), analyzer='word', max_df=1.0, \n",
    "    min_df=1, max_features=None, vocabulary=None,\n",
    "    binary=False, dtype=<class 'numpy.int64'>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',]\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "transformer = TfidfTransformer()\n",
    "transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1 = transformer.fit_transform(X)\n",
    "X1.toarray()\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.43877674, 0.54197657, 0.43877674, 0.        ,\n",
       "        0.        , 0.35872874, 0.        , 0.43877674],\n",
       "       [0.        , 0.27230147, 0.        , 0.27230147, 0.        ,\n",
       "        0.85322574, 0.22262429, 0.        , 0.27230147],\n",
       "       [0.55280532, 0.        , 0.        , 0.        , 0.55280532,\n",
       "        0.        , 0.28847675, 0.55280532, 0.        ],\n",
       "       [0.        , 0.43877674, 0.54197657, 0.43877674, 0.        ,\n",
       "        0.        , 0.35872874, 0.        , 0.43877674]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2 = vectorizer.fit_transform(corpus)\n",
    "X2\n",
    "X2.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 该代码仅为演示函数签名和所用方法，并不能实际运行\n",
    "\"\"\"\n",
    "\n",
    "class sklearn.feature_extraction.text.TfidfTransformer(norm='l2',\n",
    "    use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
    "\n",
    "class sklearn.feature_extraction.text.TfidfVectorizer(input='content',\n",
    "    encoding='utf-8', decode_error='strict', strip_accents=None,\n",
    "    lowercase=True, preprocessor=None, tokenizer=None, \n",
    "    analyzer='word',\n",
    "    stop_words=None, token_pattern='(?u)\\b\\w\\w+\\b', \n",
    "    ngram_range=(1, 1),\n",
    "    max_df=1.0, min_df=1, max_features=None, vocabulary=None,\n",
    "    binary=False, dtype=<class 'numpy.float64'>, norm='l2', use_idf=True,\n",
    "    smooth_idf=True, sublinear_tf=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 该代码仅为演示函数签名和所用方法，并不能实际运行\n",
    "\"\"\"\n",
    "\n",
    "class torch.nn.Embedding(num_embeddings, embedding_dim, \n",
    "    padding_idx=None, max_norm=None, norm_type=2.0,\n",
    "    scale_grad_by_freq=False, sparse=False, _weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0225, -0.3995, -0.2486, -0.9889],\n",
       "        [-1.6584, -0.7807,  0.5444,  0.4844],\n",
       "        [-0.0918,  1.0300,  0.8720, -0.5216],\n",
       "        [-0.3996,  0.9642, -0.3544, -1.8949],\n",
       "        [-0.4969,  1.3094, -1.2117, -2.6379],\n",
       "        [ 0.6736,  0.1839, -1.7677, -0.1652],\n",
       "        [ 0.8999,  1.1953, -0.0418,  0.7600],\n",
       "        [ 1.2768, -0.9379, -0.3259,  0.2652],\n",
       "        [ 0.0371,  0.9771,  1.1992, -2.2988],\n",
       "        [-0.5315,  1.4848, -0.1074, -0.4707]], requires_grad=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EMBEDDING\n",
    "embedding = nn.Embedding(10, 4)\n",
    "embedding.weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.6584, -0.7807,  0.5444,  0.4844],\n",
       "         [-0.0918,  1.0300,  0.8720, -0.5216],\n",
       "         [-0.4969,  1.3094, -1.2117, -2.6379],\n",
       "         [ 0.6736,  0.1839, -1.7677, -0.1652]],\n",
       "\n",
       "        [[-0.4969,  1.3094, -1.2117, -2.6379],\n",
       "         [-0.3996,  0.9642, -0.3544, -1.8949],\n",
       "         [-0.0918,  1.0300,  0.8720, -0.5216],\n",
       "         [-0.5315,  1.4848, -0.1074, -0.4707]]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.LongTensor([[1,2,4,5],[4,3,2,9]])\n",
    "embedding(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.2767, -0.5174, -1.0029, -0.0819],\n",
       "        [-0.6579, -1.5963,  0.9846,  0.0680],\n",
       "        [ 0.0622,  0.9025, -1.5875, -2.0880],\n",
       "        [ 0.3709, -0.8253,  1.8708,  1.5155],\n",
       "        [ 0.1345,  1.4424,  0.9984,  0.2505],\n",
       "        [-2.0006, -1.6429,  0.0999,  0.3841],\n",
       "        [ 1.2444, -0.2286, -1.7390, -0.1508],\n",
       "        [ 0.2757,  1.6579,  0.7066,  0.1593],\n",
       "        [-0.4578, -1.2670, -1.1276, -0.5374]], requires_grad=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = nn.Embedding(10, 4, padding_idx=0) # 定义10×4的词嵌入张量，其中索引为0的词向量为0\n",
    "embedding.weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.6579, -1.5963,  0.9846,  0.0680],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.1345,  1.4424,  0.9984,  0.2505]]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.LongTensor([[0,2,0,5]])\n",
    "embedding(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.LongTensor([[1,2,4,5],[4,3,2,9]])\n",
    "embedding(input)\n",
    "embedding = nn.Embedding(10, 4, padding_idx=0) # 定义10×4的词嵌入张量，其中索引为0的词向量为0\n",
    "embedding.weight\n",
    "input = torch.LongTensor([[0,2,0,5]])\n",
    "embedding(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 该代码仅为演示函数签名和所用方法，并不能实际运行\n",
    "\"\"\"\n",
    "\n",
    "# classmothod：类方法，返回nn.Embedding类的实例\n",
    "torch.nn.Embedding.from_pretrained(embeddings, freeze=True,\n",
    "    padding_idx=None, max_norm=None, norm_type=2.0, \n",
    "    scale_grad_by_freq=False, sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 该代码仅为演示函数签名和所用方法，并不能实际运行\n",
    "\"\"\"\n",
    "\n",
    "class torch.nn.EmbeddingBag(num_embeddings, embedding_dim, \n",
    "    max_norm=None, norm_type=2.0, scale_grad_by_freq=False,\n",
    "    mode='mean', sparse=False, _weight=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 该代码仅为演示函数签名和所用方法，并不能实际运行\n",
    "\"\"\"\n",
    "\n",
    "torch.nn.utils.rnn.pack_padded_sequence(input, lengths, \n",
    "    batch_first=False, enforce_sorted=True)\n",
    "torch.nn.utils.rnn.pad_packed_sequence(sequence, batch_first=False,\n",
    "    padding_value=0.0, total_length=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 该代码仅为演示函数签名和所用方法，并不能实际运行\n",
    "\"\"\"\n",
    "\n",
    "class torch.nn.RNN(input_size, hidden_size, num_layers,\n",
    "    nonlinearity='tanh', bias=True, batch_first=False,\n",
    "    dropout=0, bidirectional=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "rnn = nn.RNN(10, 20, 2)\n",
    "input = torch.randn(5, 3, 10)\n",
    "h0 = torch.randn(2, 3, 20)\n",
    "output, hn = rnn(input, h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 该代码仅为演示函数签名和所用方法，并不能实际运行\n",
    "\"\"\"\n",
    "\n",
    "# LSTM模块的参数\n",
    "class torch.nn.LSTM(input_size, hidden_size, num_layers,\n",
    "    bias=True, batch_first=False, dropout=0, bidirectional=False)\n",
    "\n",
    "# GRU模块的参数\n",
    "class torch.nn.GRU(input_size, hidden_size, num_layers,\n",
    "    bias=True, batch_first=False, dropout=0, bidirectional=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# LSTM模块的使用\n",
    "rnn = nn.LSTM(10, 20, 2)\n",
    "input = torch.randn(5, 3, 10)\n",
    "h0 = torch.randn(2, 3, 20)\n",
    "c0 = torch.randn(2, 3, 20)\n",
    "output, (hn, cn) = rnn(input, (h0, c0))\n",
    "\n",
    "# GRU模块的使用\n",
    "rnn = nn.GRU(10, 20, 2)\n",
    "input = torch.randn(5, 3, 10)\n",
    "h0 = torch.randn(2, 3, 20)\n",
    "output, hn = rnn(input, h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 该代码仅为演示函数签名和所用方法，并不能实际运行\n",
    "\"\"\"\n",
    "\n",
    "class torch.nn.RNNCell(input_size, hidden_size, bias=True,\n",
    "    nonlinearity='tanh')\n",
    "class torch.nn.LSTMCell(input_size, hidden_size, bias=True)\n",
    "class torch.nn.GRUCell(input_size, hidden_size, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rnn = nn.RNNCell(10, 20)\n",
    "input = torch.randn(6, 3, 10)\n",
    "hx = torch.randn(3, 20)\n",
    "output = []\n",
    "for i in range(6):\n",
    "    hx = rnn(input[i], hx)\n",
    "    output.append(hx)\n",
    "\n",
    "rnn = nn.LSTMCell(10, 20)\n",
    "input = torch.randn(6, 3, 10)\n",
    "hx = torch.randn(3, 20)\n",
    "cx = torch.randn(3, 20)\n",
    "output = []\n",
    "for i in range(6):\n",
    "    hx, cx = rnn(input[i], (hx, cx))\n",
    "    output.append(hx)\n",
    "\n",
    "rnn = nn.GRUCell(10, 20)\n",
    "input = torch.randn(6, 3, 10)\n",
    "hx = torch.randn(3, 20)\n",
    "output = []\n",
    "for i in range(6):\n",
    "    hx = rnn(input[i], hx)\n",
    "    output.append(hx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 该代码仅为演示函数签名和所用方法，并不能实际运行\n",
    "\"\"\"\n",
    "\n",
    "class torch.nn.MultiheadAttention(embed_dim, num_heads,\n",
    "    dropout=0.0, bias=True, add_bias_kv=False,\n",
    "    add_zero_attn=False, kdim=None, vdim=None)\n",
    "\n",
    "# 对应的forward方法定义\n",
    "forward(query, key, value, key_padding_mask=None,\n",
    "    need_weights=True, attn_mask=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 该代码仅为演示函数签名和所用方法，并不能实际运行\n",
    "\"\"\"\n",
    "\n",
    "class torch.nn.TransformerEncoderLayer(d_model,\n",
    "    nhead, dim_feedforward=2048, dropout=0.1)\n",
    "# TransformerEncoderLayer对应的forward方法定义\n",
    "forward(src, src_mask=None, src_key_padding_mask=None)\n",
    "\n",
    "class torch.nn.TransformerDecoderLayer(d_model,\n",
    "    nhead, dim_feedforward=2048, dropout=0.1)\n",
    "# TransformerDecoderLayer对应的forward方法定义\n",
    "forward(tgt, memory, tgt_mask=None, memory_mask=None,\n",
    "    tgt_key_padding_mask=None, memory_key_padding_mask=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 该代码仅为演示函数签名和所用方法，并不能实际运行\n",
    "\"\"\"\n",
    "\n",
    "class torch.nn.TransformerEncoder(encoder_layer, num_layers, norm=None)\n",
    "# TransformerEncoder对应的forward方法定义\n",
    "forward(src, mask=None, src_key_padding_mask=None)\n",
    "\n",
    "class torch.nn.TransformerDecoder(decoder_layer, num_layers, norm=None)\n",
    "# TransformerDecoder对应的forward方法定义\n",
    "forward(tgt, memory, tgt_mask=None, memory_mask=None,\n",
    "    tgt_key_padding_mask=None, memory_key_padding_mask=None)\n",
    "\n",
    "class torch.nn.Transformer(d_model=512, nhead=8, num_encoder_layers=6,\n",
    "    num_decoder_layers=6, dim_feedforward=2048, dropout=0.1,\n",
    "    custom_encoder=None, custom_decoder=None)\n",
    "# Transformer对应的forward方法定义\n",
    "forward(src, tgt, src_mask=None, tgt_mask=None, memory_mask=None,\n",
    "    src_key_padding_mask=None, tgt_key_padding_mask=None,\n",
    "    memory_key_padding_mask=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 本示例所用的代码可以被用于NLP项目中用于统计词频，构建单词表\n",
    "\"\"\"\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "class Vocab(object):\n",
    "\n",
    "    UNK = '<unk>'\n",
    "\n",
    "    def __init__(self, counter, max_size=None, min_freq=1,\n",
    "                 specials=['<unk>', '<pad>'], specials_first=True):\n",
    "\n",
    "        self.freqs = counter\n",
    "        counter = counter.copy()\n",
    "        min_freq = max(min_freq, 1)\n",
    "\n",
    "        # 定义整数序号到单词映射\n",
    "        self.itos = list()\n",
    "        self.unk_index = None\n",
    "        if specials_first:\n",
    "            self.itos = list(specials)\n",
    "            max_size = None if max_size is None else max_size + len(specials)\n",
    "\n",
    "        # 如果输入有特殊字符，删掉这些特殊字符\n",
    "        for tok in specials:\n",
    "            del counter[tok]\n",
    "\n",
    "        # 先按照字母顺序排序，再按照频率排序\n",
    "        words_and_frequencies = sorted(counter.items(), \\\n",
    "                                       key=lambda tup: tup[0])\n",
    "        words_and_frequencies.sort(key=lambda tup: tup[1], reverse=True)\n",
    "\n",
    "        # 排除小频率单词\n",
    "        for word, freq in words_and_frequencies:\n",
    "            if freq < min_freq or len(self.itos) == max_size:\n",
    "                break\n",
    "            self.itos.append(word)\n",
    "\n",
    "        if Vocab.UNK in specials: \n",
    "            unk_index = specials.index(Vocab.UNK)\n",
    "            self.unk_index = unk_index if specials_first \\\n",
    "                else len(self.itos) + unk_index\n",
    "            self.stoi = defaultdict(self._default_unk_index)\n",
    "        else:\n",
    "            self.stoi = defaultdict()\n",
    "\n",
    "        if not specials_first:\n",
    "            self.itos.extend(list(specials))\n",
    "\n",
    "        # 定义单词到整数序号映射\n",
    "        self.stoi.update({tok: i for i, tok in enumerate(self.itos)})\n",
    "\n",
    "def build_vocab_from_iterator(iterator):\n",
    "\n",
    "    counter = Counter()\n",
    "    for tokens in iterator:\n",
    "        counter.update(tokens)\n",
    "    word_vocab = Vocab(counter)\n",
    "    return word_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BERT构建\n",
    "# 嵌入层\n",
    "class BertEmbeddings(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BertEmbeddings, self).__init__()\n",
    "        # 单词的词嵌入\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, \n",
    "                                            config.hidden_size, padding_idx=0)\n",
    "        # 位置的嵌入\n",
    "        self.position_embeddings = nn.Embedding(\\\n",
    "                                        config.max_position_embeddings,\n",
    "                                        config.hidden_size)\n",
    "        # 片段的词嵌入\n",
    "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size,\n",
    "                                                          config.hidden_size)\n",
    "        # 层归一化\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size,\n",
    "                                            eps=config.layer_norm_eps)\n",
    "        # 丢弃层\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, position_ids=None):\n",
    "        # 假设模型的输入input_ids的大小是L×N，其中L为最大序列长度，\n",
    "        # N为迷你批次大小\n",
    "        seq_length = input_ids.size(0)\n",
    "        if position_ids is None:\n",
    "            position_ids = torch.arange(seq_length, \n",
    "                                        dtype=torch.long, device=input_ids.device)\n",
    "            position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "\n",
    "        words_embeddings = self.word_embeddings(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "\n",
    "        embeddings = words_embeddings + position_embeddings + \\\n",
    "                     token_type_embeddings\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "# BERT编码器\n",
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BertEncoder, self).__init__()\n",
    "        self.output_attentions = config.output_attentions\n",
    "        self.output_hidden_states = config.output_hidden_states\n",
    "        self.layer = nn.ModuleList([nn.TransformerEncoderLayer(\n",
    "                                        config.hidden_size,\n",
    "                                        config.num_attention_heads,\n",
    "                                        config.intermediate_size,\n",
    "                                        config.hidden_dropout_prob) \\\n",
    "                                    for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None, head_mask=None):\n",
    "        all_hidden_states = ()\n",
    "        all_attentions = ()\n",
    "        # 迭代计算中间的输出\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            if self.output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            layer_outputs = layer_module(hidden_states, attention_mask, \n",
    "                                         head_mask)\n",
    "            hidden_states = layer_outputs[0]\n",
    "\n",
    "            if self.output_attentions:\n",
    "                all_attentions = all_attentions + (layer_outputs[1],)\n",
    "\n",
    "        if self.output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        outputs = (hidden_states,)\n",
    "        if self.output_hidden_states:\n",
    "            outputs = outputs + (all_hidden_states,)\n",
    "        if self.output_attentions:\n",
    "            outputs = outputs + (all_attentions,)\n",
    "        # 包含最后的输出、中间的输出，以及自注意力的权重\n",
    "        return outputs  \n",
    "\n",
    "# BERT预训练模型\n",
    "class BertPretrainModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BertPretrainModel, self).__init__()\n",
    "        self.embedding = BertEmbeddings(config)\n",
    "        self.bert = BertEncoder(config)\n",
    "        self.fc = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activ1 = nn.Tanh()\n",
    "        self.linear = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activ2 = nn.ReLU()\n",
    "        self.norm = nn.LayerNorm(config.hidden_size,\n",
    "                                 eps=config.layer_norm_eps)\n",
    "        self.classifier = nn.Linear(config.hidden_size, 2)\n",
    "        embed_weight = self.transformer.embed.tok_embed.weight\n",
    "        n_vocab, n_dim = embed_weight.size()\n",
    "        self.decoder = nn.Linear(n_dim, n_vocab, bias=False)\n",
    "        self.decoder.weight = embed_weight\n",
    "        self.decoder_bias = nn.Parameter(torch.zeros(n_vocab))\n",
    "\n",
    "    def forward(self, input_ids, segment_ids, input_mask, masked_pos):\n",
    "\n",
    "        embed = self.embedding(input_ids, segment_ids)\n",
    "        h = self.bert(embed, input_mask, masked_pos)\n",
    "        pooled_h = self.activ1(self.fc(h[:, 0]))\n",
    "        masked_pos = masked_pos[:, :, None].expand(-1, -1, h.size(-1))\n",
    "        h_masked = torch.gather(h, 1, masked_pos)\n",
    "        h_masked = self.norm(self.activ2(self.linear(h_masked)))\n",
    "        logits_lm = self.decoder(h_masked) + self.decoder_bias\n",
    "        logits_clsf = self.classifier(pooled_h)\n",
    "\n",
    "        return logits_lm, logits_clsf\n",
    "\n",
    "# BERT机器阅读理解任务\n",
    "class BertQA(nn.Module)\n",
    "    def __init__(self, config):\n",
    "        super(BertQA, self).__init__()\n",
    "        self.embedding = BertEmbeddings(config)\n",
    "        self.bert = BertEncoder(config)\n",
    "\n",
    "        self.start = nn.Parameter(torch.randn(1, 1, config.hidden_size))\n",
    "        self.end = nn.Parameter(torch.randn(1, 1, config.hidden_size))\n",
    "\n",
    "        self.fc = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activ1 = nn.Tanh()\n",
    "    \n",
    "    def forward(self, input_ids, segment_ids, input_mask, masked_pos):\n",
    "\n",
    "        embed = self.embedding(input_ids, segment_ids)\n",
    "        h = self.bert(embed, input_mask, masked_pos)\n",
    "        h = self.activ1(self.fc(h))\n",
    "\n",
    "        logits_start = (h*self.start).sum(-1)\n",
    "        logits_end = (h*self.end).sum(-1)\n",
    "\n",
    "        return logits_start, logits_end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 以下代码仅作为循环神经网络语言模型的实现参考\n",
    "\"\"\"\n",
    "# 模型\n",
    "class LM(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhid, nlayers, dropout=0.5,\n",
    "                 tie_weights=False):\n",
    "        super(LM, self).__init__()\n",
    "\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "\n",
    "        if tie_weights:\n",
    "            if nhid != ninp:\n",
    "                raise ValueError('When using the tied flag, \\\n",
    "                                  nhid must be equal to emsize')\n",
    "            self.decoder.weight = self.encoder.weight\n",
    "\n",
    "        self.init_weights()\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        emb = self.drop(self.encoder(input))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output)\n",
    "        return decoded, hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters())\n",
    "        return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
    "                weight.new_zeros(self.nlayers, bsz, self.nhid)\n",
    "\n",
    "# 训练代码\n",
    "model = LM(ntokens, ninp, nhid, nlayers, dropout, tie_weights).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    model.train()\n",
    "    # 获取单词表大小\n",
    "    ntokens = len(vocab)\n",
    "\n",
    "    for data, targets in train_data:\n",
    "        # data和targets的形状大小都为 L×N,\n",
    "        # 其中L为序列长度， N为批次大小\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        model.zero_grad()\n",
    "        hidden = model.init_hidden(data.size(1))\n",
    "        output, hidden = model(data, hidden)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "\n",
    "    return model\n",
    "\n",
    "# 预测代码\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    hidden = model.init_hidden(1)\n",
    "    input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n",
    "    words = []\n",
    "    for i in range(max_words):\n",
    "        output, hidden = model(input, hidden)\n",
    "        word_weights = output.squeeze().softmax(-1).cpu()\n",
    "        word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "        input.fill_(word_idx)\n",
    "        word = vocab.itos[word_idx]\n",
    "        words.append(word)\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 以下代码仅作为情感分析模型的实现参考\n",
    "\"\"\"\n",
    "\n",
    "class Sentiment(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim,\n",
    "                 n_layers, bidirectional, dropout, pad_idx):\n",
    "        \n",
    "        super(Sentiment, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim,\n",
    "                                      padding_idx = pad_idx)\n",
    "        self.rnn = nn.LSTM(embedding_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim) \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded,\n",
    "                                                            text_lengths)\n",
    "        \n",
    "        packed_output, (hidden, cell) = self.rnn(packed_embedded)        \n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), im = 1))     \n",
    "        return self.fc(hidden)\n",
    "\n",
    "\n",
    "model = Sentiment(vocab_size, embedding_dim, hidden_dim, output_dim,\n",
    "                     n_layers, bidirectional, dropout, pad_idx)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:        \n",
    "        optimizer.zero_grad()\n",
    "        text, text_lengths = batch.text  \n",
    "        predictions = model(text, text_lengths).squeeze(1)   \n",
    "        loss = criterion(predictions, batch.label)    \n",
    "        loss.backward()       \n",
    "        optimizer.step()\n",
    "        \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 本代码仅作为Seq2Seq模型的实现参考\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# 编码器\n",
    "class LSTMEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, dictionary,embed_dim=512, hidden_size=512, num_layers=1,\n",
    "        dropout_in=0.1, dropout_out=0.1, bidirectional=False,\n",
    "        padding_value=0.):\n",
    "\n",
    "        super(LSTMEncoder, self).__init__()\n",
    "        self.dictionary = dictionary\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_in = dropout_in\n",
    "        self.dropout_out = dropout_out\n",
    "        self.bidirectional = bidirectional\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        num_embeddings = len(dictionary)\n",
    "        # 获取单词表中'<PAD>'单词对应的整数索引\n",
    "        self.padding_idx = dictionary.pad()\n",
    "        self.embed_tokens = Embedding(num_embeddings, embed_dim,\n",
    "                                      self.padding_idx)\n",
    "\n",
    "        self.lstm = LSTM(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=self.dropout_out if num_layers > 1 else 0.,\n",
    "            bidirectional=bidirectional,\n",
    "        )\n",
    "        self.padding_value = padding_value\n",
    "\n",
    "        self.output_units = hidden_size\n",
    "        if bidirectional:\n",
    "            self.output_units *= 2\n",
    "\n",
    "    def forward(self, src_tokens, src_lengths):\n",
    "        # 假设输入为 L×N，其中L为最大序列长度，N为迷你批次大小\n",
    "        seqlen, bsz = src_tokens.size()\n",
    "        # 查找输入序列对应的词嵌入\n",
    "        x = self.embed_tokens(src_tokens)\n",
    "        x = F.dropout(x, p=self.dropout_in, training=self.training)\n",
    "        # 对输入张量进行打包\n",
    "        packed_x = nn.utils.rnn.pack_padded_sequence(x,\n",
    "                                                     src_lengths.data.tolist())\n",
    "        # 获取隐含状态大小\n",
    "        if self.bidirectional:\n",
    "            state_size = 2 * self.num_layers, bsz, self.hidden_size\n",
    "        else:\n",
    "            state_size = self.num_layers, bsz, self.hidden_size\n",
    "        # 初始化隐含状态为全零张量\n",
    "        h0 = x.new_zeros(*state_size)\n",
    "        c0 = x.new_zeros(*state_size)\n",
    "        # 根据输入张量计算LSTM输出\n",
    "        packed_outs, (final_hiddens, final_cells) = self.lstm(packed_ x, (h0, c0))\n",
    "        # 对LSTM输出进行解包\n",
    "        x, _ = nn.utils.rnn.pad_packed_sequence(packed_outs,\n",
    "                                                padding_value=self.padding_value)\n",
    "        x = F.dropout(x, p=self.dropout_out, training=self.training)\n",
    "\n",
    "        # 融合双向LSTM的维度\n",
    "        if self.bidirectional:\n",
    "            def combine_bidir(outs):\n",
    "                out = outs.view(self.num_layers, 2, bsz, -1)\\\n",
    "                        .transpose(1, 2).contiguous()\n",
    "                return out.view(self.num_layers, bsz, -1)\n",
    "\n",
    "            final_hiddens = combine_bidir(final_hiddens)\n",
    "            final_cells = combine_bidir(final_cells)\n",
    "\n",
    "        encoder_padding_mask = src_tokens.eq(self.padding_idx).t()\n",
    "\n",
    "        return {\n",
    "            'encoder_out': (x, final_hiddens, final_cells),\n",
    "            'encoder_padding_mask': encoder_padding_mask \\\n",
    "                if encoder_padding_mask.any() else None\n",
    "        }\n",
    "\n",
    "# 注意力机制\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, input_embed_dim, source_embed_dim,\n",
    "                 output_embed_dim, bias=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_proj = Linear(input_embed_dim,\n",
    "                                 source_embed_dim, bias=bias)\n",
    "        self.output_proj = Linear(input_embed_dim + source_embed_dim,\n",
    "                                 output_embed_dim, bias=bias)\n",
    "\n",
    "    def forward(self, input, source_hids, encoder_padding_mask):\n",
    "\n",
    "        # 假设input为 B×H，B为迷你批次的大小，H为隐含状态大小\n",
    "        # 假设source_hids为L×B×H，L为序列长度，B为迷你批次的大小，\n",
    "        # H为隐含状态大小\n",
    "        x = self.input_proj(input)\n",
    "\n",
    "        # 计算注意力分数\n",
    "        attn_scores = (source_hids * x.unsqueeze(0)).sum(dim=2)\n",
    "\n",
    "        # 设置填充单词的注意力分数为-inf\n",
    "        if encoder_padding_mask is not None:\n",
    "            attn_scores = attn_scores.float().masked_fill_(\n",
    "                encoder_padding_mask,\n",
    "                float('-inf')\n",
    "            ).type_as(attn_scores)\n",
    "\n",
    "        attn_scores = F.softmax(attn_scores, dim=0)\n",
    "\n",
    "        # 对编码器输出加权平均\n",
    "        x = (attn_scores.unsqueeze(2) * source_hids).sum(dim=0)\n",
    "\n",
    "        x = torch.tanh(self.output_proj(torch.cat((x, input), dim=1)))\n",
    "        return x, attn_scores\n",
    "\n",
    "# 解码器\n",
    "class LSTMDecoder(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, dictionary, embed_dim=512, hidden_size=512, out_embed_dim=512,\n",
    "        num_layers=1, dropout_in=0.1, dropout_out=0.1, attention=True,\n",
    "        encoder_output_units=512):\n",
    "\n",
    "        super(LSTMDecoder, self).__init__()\n",
    "        self.dictionary = dictionary\n",
    "        self.dropout_in = dropout_in\n",
    "        self.dropout_out = dropout_out\n",
    "        self.hidden_size = hidden_size\n",
    "        self.need_attn = True\n",
    "\n",
    "        num_embeddings = len(dictionary)\n",
    "        padding_idx = dictionary.pad()\n",
    "\n",
    "        self.embed_tokens = Embedding(num_embeddings, embed_dim, padding_idx)\n",
    "\n",
    "        self.encoder_output_units = encoder_output_units\n",
    "        if encoder_output_units != hidden_size:\n",
    "            self.encoder_hidden_proj = Linear(encoder_output_units,\n",
    "                                              hidden_size)\n",
    "            self.encoder_cell_proj = Linear(encoder_output_units, hidden_size)\n",
    "        else:\n",
    "            self.encoder_hidden_proj = self.encoder_cell_proj = None\n",
    "        self.layers = nn.ModuleList([\n",
    "            LSTMCell(\n",
    "                input_size=hidden_size + embed_dim \\\n",
    "                    if layer == 0 else hidden_size,\n",
    "                hidden_size=hidden_size,\n",
    "            )\n",
    "            for layer in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.attention = AttentionLayer(hidden_size, encoder_output_units,\n",
    "                                        hidden_size, bias=False)\n",
    "        self.fc_out = Linear(out_embed_dim, num_embeddings,\n",
    "                             dropout=dropout_out)\n",
    "\n",
    "    def forward(self, prev_output_tokens, encoder_out, \n",
    "                incremental_state=None):\n",
    "        encoder_padding_mask = encoder_out['encoder_padding_mask']\n",
    "        encoder_out = encoder_out['encoder_out']\n",
    "\n",
    "        # 获取保存的输出单词，用于模型的预测\n",
    "        if incremental_state is not None:\n",
    "            prev_output_tokens = prev_output_tokens[:, -1:]\n",
    "        bsz, seqlen = prev_output_tokens.size()\n",
    "\n",
    "        encoder_outs, encoder_hiddens, encoder_cells = encoder_out[:3]\n",
    "        srclen = encoder_outs.size(0)\n",
    "\n",
    "        x = self.embed_tokens(prev_output_tokens)\n",
    "        x = F.dropout(x, p=self.dropout_in, training=self.training)\n",
    "\n",
    "        # 获取保存的状态，用于模型的预测\n",
    "        cached_state = utils.get_incremental_state(self, incremental_state,\n",
    "                                                      'cached_state')\n",
    "\n",
    "        if cached_state is not None:\n",
    "            prev_hiddens, prev_cells, input_feed = cached_state\n",
    "        else:\n",
    "            num_layers = len(self.layers)\n",
    "            prev_hiddens = [encoder_hiddens[i] for i in range(num_layers)]\n",
    "            prev_cells = [encoder_cells[i] for i in range(num_layers)]\n",
    "            if self.encoder_hidden_proj is not None:\n",
    "                prev_hiddens = [self.encoder_hidden_proj(x) \\\n",
    "                                for x in prev_hiddens]\n",
    "                prev_cells = [self.encoder_cell_proj(x) for x in prev_cells]\n",
    "            input_feed = x.new_zeros(bsz, self.hidden_size)\n",
    "\n",
    "        attn_scores = x.new_zeros(srclen, seqlen, bsz)\n",
    "        outs = []\n",
    "\n",
    "        # 进行迭代的循环神经网络计算\n",
    "        for j in range(seqlen):\n",
    "            # 输入中引入上一步的注意力机制的信息\n",
    "            input = torch.cat((x[j, :, :], input_feed), dim=1)\n",
    "            # 迭代所有的循环神经网络层\n",
    "            for i, rnn in enumerate(self.layers):\n",
    "                hidden, cell = rnn(input, (prev_hiddens[i], prev_cells[i]))\n",
    "                input = F.dropout(hidden, p=self.dropout_out,\n",
    "                                  training=self.training)\n",
    "\n",
    "                prev_hiddens[i] = hidden\n",
    "                prev_cells[i] = cell\n",
    "            # 计算注意力的输出值和注意力权重\n",
    "            if self.attention is not None:\n",
    "                out, attn_scores[:, j, :] = self.attention(\n",
    "                    hidden, encoder_outs, encoder_padding_mask)\n",
    "            else:\n",
    "                out = hidden\n",
    "            out = F.dropout(out, p=self.dropout_out, training=self.training)\n",
    "            input_feed = out\n",
    "            outs.append(out)\n",
    "            \n",
    "        # 保存隐含状态\n",
    "        utils.set_incremental_state(\n",
    "            self, incremental_state, 'cached_state',\n",
    "            (prev_hiddens, prev_cells, input_feed),\n",
    "        )\n",
    "\n",
    "        x = torch.cat(outs, dim=0).view(seqlen, bsz, self.hidden_size)\n",
    "        attn_scores = attn_scores.transpose(0, 2)\n",
    "        x = self.fc_out(x)\n",
    "        return x, attn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
